# Desafio de Data Engineer - IPLANRIO

---

## Objetivo: 
O objetivo desse desafio Ã© resolver o problema proposta pela equipe do IPLANRIO para a vaga de engenheiro de Dados, criando uma arquitetura simples, resiliente e escalÃ¡vel. Para maiores informaÃ§Ãµes, leia: `https://github.com/prefeitura-rio/iplanrio-desafio-data-eng`


## DecisÃµes Arquiteturais
---

- OrquestraÃ§Ã£o com Prefect
- Bronze recriada a cada execuÃ§Ã£o devido ao baixo volume
- Silver incremental
- Gold materializada como tabela
- OrquestraÃ§Ã£o de containers com Docker Compose
- ExposiÃ§Ã£o dos dados via API REST com FastAPI

Seguindo o desafio proposto, ao final do processo de ETL, o bucket ficou da seguinte forma:

```
br-cgu-terceirizados/
â”œâ”€â”€ terceirizados/
â”‚   â”œâ”€â”€ terceirizados_2025-01.parquet
â”‚   â”œâ”€â”€ terceirizados_2025-05.parquet
â”‚   â”œâ”€â”€ terceirizados_2025-07.parquet
â”‚   â””â”€â”€ ...
â”œâ”€â”€ bronze/
â”‚   â””â”€â”€ terceirizados-bronze.duckdb
â”œâ”€â”€ silver/
â”‚   â””â”€â”€ terceirizados-silver.duckdb
â””â”€â”€ gold/
    â””â”€â”€ terceirizados-gold.duckdb
```


## Arquiteutra do Projeto

![Texto Alternativo](images/arquitetura.png)

## Configurando o projeto

## Configurando o Projeto

### 1. Clone o repositÃ³rio
```bash
git clone https://github.com/tricktx/iplanrio-desafio-data-eng.git
```

### 2. Navegue atÃ© o diretÃ³rio do projeto
```bash
cd iplanrio-desafio-data-eng
```

### 3. Configure as variÃ¡veis de ambiente

Crie um arquivo `.env` na raiz do projeto com a seguinte variÃ¡vel:
```env
export GOOGLE_APPLICATION_CREDENTIALS=</path/service/account.json>
```

> Substitua `</path/service/account.json>` pelo caminho real da sua service account do GCP.

### 4. Suba os containers com Docker Compose
```bash
docker compose up -d --build
```

### 5. Acesse as interfaces

ApÃ³s todos os containers estarem ativos, acesse:

- **Prefect UI** â†’ [http://127.0.0.1:4200/dashboard](http://127.0.0.1:4200/dashboard)
- **FastAPI** â†’ [http://localhost:8000/](http://localhost:8000/)

### 6. Configure os Blocks no Prefect

Na UI do Prefect, acesse a aba **Blocks** e configure:

- `cgu-bucket` â†’ nome do seu bucket no GCP
- `cgu-service-account` â†’ caminho da sua service account

> **ðŸ’¡ Dica:** Caso queira criar um novo bucket, utilize o Terraform disponÃ­vel em `terraform/main.tf` e atualize o nome do bucket dentro dos blocks apÃ³s a criaÃ§Ã£o.
---

## Fluxo dos Dados

## OrquestraÃ§Ã£o

O **Prefect** executa a pipeline diariamente Ã s **19:00 (horÃ¡rio de BrasÃ­lia)**.

- **Flow:** `CGU Data Pipeline`  
- **DefiniÃ§Ã£o:** `src.pipelines.flows.py`  
- **Deploy:** `deploy.py`  
- **Nome do deployment:** `deploy-cgu`  

> [!NOTE]  
> Se for a primeira execuÃ§Ã£o do projeto, recomenda-se fortemente rodar a pipeline com o parÃ¢metro `load_to_data=True`.  
>  
> Nesse modo, o bucket serÃ¡ populado com todos os dados histÃ³ricos disponÃ­veis, executando integralmente o processo de ingestÃ£o, validaÃ§Ã£o e particionamento.

---

## Estrutura das Tasks (`src.pipelines.tasks`)

A pipeline Ã© composta por tasks responsÃ¡veis por controlar disponibilidade, ingestÃ£o e consistÃªncia dos dados.

### 1. `check_for_updates`

ResponsÃ¡vel por:

- Consultar a **data mÃ¡xima** disponÃ­vel na camada Bronze.
- Adicionar **4 meses** Ã  data encontrada (janela em que os dados costumam ser publicados).
- Construir dinamicamente a URL de verificaÃ§Ã£o.
- Validar se a requisiÃ§Ã£o HTTP retorna **status 200**.

**Comportamento:**

- Se retornar `200`, os dados sÃ£o considerados disponÃ­veis e o download Ã© iniciado.
- Caso contrÃ¡rio, a execuÃ§Ã£o Ã© encerrada de forma controlada e o flow serÃ¡ reexecutado no prÃ³ximo agendamento.

Foi implementada ainda uma lÃ³gica para impedir carga duplicada no banco, garantindo **idempotÃªncia** no processo.

---

### 2. `ingest_and_partition`

ResponsÃ¡vel pela ingestÃ£o e padronizaÃ§Ã£o dos dados.

Principais etapas:

- ValidaÃ§Ã£o estrutural dos arquivos.
- CorreÃ§Ã£o de inconsistÃªncias histÃ³ricas (exemplo: `201901`, que nÃ£o continha cabeÃ§alho de colunas).
- Garantia de padronizaÃ§Ã£o do schema antes da persistÃªncia.

ApÃ³s validaÃ§Ã£o, os dados sÃ£o salvos em formato **`.parquet`**, particionados por ano e mÃªs:

```
terceirizados_201901
terceirizados_201902
```

Essa estratÃ©gia melhora organizaÃ§Ã£o, rastreabilidade e performance de leitura.

---

## UtilitÃ¡rios (`src.utils.setup`)

### 1. `upload_files_in_directory`

Realiza o upload de todos os arquivos de um diretÃ³rio local para uma pasta especÃ­fica no **GCS**.

### 2. ExecuÃ§Ã£o do dbt

Executa:

- `dbt run` para as camadas:
  - Bronze
  - Silver
  - Gold
- `dbt test` na camada Silver

A camada **Silver** utiliza materializaÃ§Ã£o **incremental com chave tÃ©cnica**, evitando reprocessamento completo e garantindo eficiÃªncia.

---

> [!NOTE]  
> Caso deseje executar a pipeline para anos anteriores, consulte a documentaÃ§Ã£o da funÃ§Ã£o `check_for_update_and_download`.  
>  
> NÃ£o hÃ¡ previsÃ£o oficial de atualizaÃ§Ã£o retroativa na fonte. Portanto:
>  
> - A camada **Silver** nÃ£o serÃ¡ reprocessada integralmente (modelo incremental).  
> - A camada **Gold**, derivada da Silver, tambÃ©m nÃ£o sofrerÃ¡ alteraÃ§Ãµes.  
>  
> ExecuÃ§Ãµes retroativas possuem finalidade demonstrativa, evidenciando a consistÃªncia e reprodutibilidade da arquitetura.

---

## ExposiÃ§Ã£o da Camada Gold via API

A camada **Gold** Ã© exposta por meio de uma API REST construÃ­da com **FastAPI**.

### Endpoints disponÃ­veis

**PaginaÃ§Ã£o:**
```bash
http://localhost:8000/terceirizados/pages/{page}
```


**Consulta por ID:**
```
http://localhost:8000/terceirizados/{id}
```

A API implementa paginaÃ§Ã£o e filtros diretamente na base, garantindo eficiÃªncia no consumo dos dados.

Percebe-se na imagem abaixo que o fluxo de Dados rodou perfeitamente no Prefect 3.
![alt text](images/image.png)

TambÃ©m podemos validar a pÃ¡gina de page e de id no FastAPI retornando os dados:
![alt text](images/image-1.png)

![alt text](images/image-2.png)